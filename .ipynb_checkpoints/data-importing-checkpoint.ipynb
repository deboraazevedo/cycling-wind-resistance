{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Project - Unit 2\n",
    "*by Débora Azevedo, Eliseu Jayro, Francisco de Paiva and Igor Brandão*\n",
    "\n",
    "**Goals**\n",
    "The purpose of this project is explore the following:\n",
    "\n",
    "- Access Health Graph API - Runkeeper content;\n",
    "- Geolocation analysis and hypotheses should be explained in detail;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports section\n",
    "\n",
    "Import the necessary libraries to handle \n",
    "\n",
    "- File input;\n",
    "- Tqdm progress bar\n",
    "- Requests;\n",
    "- urlopen;\n",
    "- HTTPError;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (4.26.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.8.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-datareader in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: lxml in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas-datareader) (4.2.5)\n",
      "Requirement already satisfied: pandas>=0.19.2 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas-datareader) (0.23.4)\n",
      "Requirement already satisfied: wrapt in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas-datareader) (1.10.11)\n",
      "Requirement already satisfied: requests>=2.3.0 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas-datareader) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->pandas-datareader) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->pandas-datareader) (2018.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->pandas-datareader) (1.15.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.3.0->pandas-datareader) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.3.0->pandas-datareader) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.3.0->pandas-datareader) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.3.0->pandas-datareader) (2018.8.24)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.19.2->pandas-datareader) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in d:\\users\\f538118\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests) (1.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "### Library necessary to run this IPython Notebook\n",
    "!pip install tqdm\n",
    "!pip install tabulate\n",
    "!pip install pandas-datareader\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import tqdm progressing bar plugin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import API libraries\n",
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Imports to output the result as a Markdown\n",
    "from IPython.display import display, Markdown\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - API section\n",
    "\n",
    "## API data retrieving\n",
    "\n",
    "#### In the cell bellow, we perform a connection with Health Graph API - Runkeeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access token\n",
    "ACCESS_TOKEN = '25bc30d6dd6f4b99bbeb48e8619103b4'\n",
    "\n",
    "# Base URI\n",
    "api_URI = \"http://api.runkeeper.com/fitnessActivities\"\n",
    "\n",
    "# Number of results\n",
    "pageSize = 5000\n",
    "\n",
    "# Final URI\n",
    "url = '%s?pageSize=%s&access_token=%s' % \\\n",
    "    (api_URI, pageSize, ACCESS_TOKEN,)\n",
    "\n",
    "# print(url)\n",
    "\n",
    "# Receive the results from API\n",
    "api_content = requests.get(url).json()\n",
    "\n",
    "print(json.dumps(api_content, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to Data Frame conversion\n",
    "\n",
    "In order to have a better data manipulation, in the next cell we perform a conversion of importe in json format from API to pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a conversion from JSON to Data Frame\n",
    "api_df = json_normalize(api_content['items'])\n",
    "\n",
    "# Converts the duration from seconds to minutes\n",
    "api_df['duration'] = api_df['duration']/60\n",
    "api_df['duration'] = api_df['duration'].round(2);\n",
    "\n",
    "# Round the distance\n",
    "api_df['total_distance'] = api_df['total_distance'].round(2);\n",
    "\n",
    "api_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export [optional]\n",
    "\n",
    "In order to visualize the data into an excel file, the cell bellow is responsible for exporting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the new dataSet to csv\n",
    "api_df.to_csv('dataSource.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Activites by period\n",
    "\n",
    "#### First of all we need to split the information by period. To achieve that, the idea is apply a group selection by partial string in timestamp column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp split into columns\n",
    "\n",
    "In order to have a better way to handle the data by period, it'll be necessary split the timestamp column into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Dataframe timestamp split\n",
    "# =================================================================================\n",
    "\n",
    "# Copy the data\n",
    "data_by_period = api_df.copy()\n",
    "data_by_period[\"month\"] = 0\n",
    "data_by_period[\"month_index\"] = 0\n",
    "data_by_period[\"year\"] = 0\n",
    "\n",
    "# Fill the years\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('2018'), 'year'] = '2018'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('2017'), 'year'] = '2017'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('2016'), 'year'] = '2016'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('2015'), 'year'] = '2015'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('2014'), 'year'] = '2014'\n",
    "\n",
    "# Fill the months\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jan'), 'month'] = 'Jan'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jan'), 'month_index'] = '1'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Feb'), 'month'] = 'Feb'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Feb'), 'month_index'] = '2'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Mar'), 'month'] = 'Mar'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Mar'), 'month_index'] = '3'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Apr'), 'month'] = 'Apr'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Apr'), 'month_index'] = '4'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('May'), 'month'] = 'May'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('May'), 'month_index'] = '5'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jun'), 'month'] = 'Jun'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jun'), 'month_index'] = '6'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jul'), 'month'] = 'Jul'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Jul'), 'month_index'] = '7'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Aug'), 'month'] = 'Aug'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Aug'), 'month_index'] = '8'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Sep'), 'month'] = 'Sep'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Sep'), 'month_index'] = '9'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Oct-'), 'month'] = 'Oct'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Oct'), 'month_index'] = '10'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Nov'), 'month'] = 'Nov'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Nov'), 'month_index'] = '11'\n",
    "\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Dec'), 'month'] = 'Dec'\n",
    "data_by_period.loc[data_by_period['start_time'].str.contains('Dec'), 'month_index'] = '12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export [optional]\n",
    "\n",
    "In order to avoid replacing the timestamp to other columns, we'll export the new dataSet to a csv file. You can skip this operation because the new .csv dataSet is already included in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the new dataSet to csv\n",
    "data_by_period.to_csv('dataSourceByPeriod.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the period [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter param\n",
    "year = '2018'\n",
    "\n",
    "# Copy the data with timestamp splitted\n",
    "data_filter = data_by_period.copy()\n",
    "\n",
    "# Perform the filter\n",
    "data_filter = data_filter[data_filter['year'] == year]\n",
    "\n",
    "# Export the new dataSet to csv\n",
    "data_filter.to_csv('dataSourceByPeriod' + year + '.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Geolocation section\n",
    "\n",
    "#### Here in this section, we'll handle the geolocation infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocation data import [warning: too heavy]\n",
    "\n",
    "The cell below perform multiple requests to Runkeep Health Graph API, in order to retrieve all activities in details to get location informations such as: latitude/longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Geolocation data\n",
    "# =================================================================================\n",
    "\n",
    "# Copy the data\n",
    "geolocation_data = pd.DataFrame()\n",
    "\n",
    "# =================================================================================\n",
    "# API single activity request\n",
    "# =================================================================================\n",
    "\n",
    "# Base URI\n",
    "base_URI = \"http://api.runkeeper.com\"\n",
    "\n",
    "# Run through all activites\n",
    "for idx, row in tqdm(api_df.iterrows()):\n",
    "    \n",
    "    # Final activity URI\n",
    "    activity_url = base_URI + row['uri'] + \"?access_token=\" + ACCESS_TOKEN\n",
    "\n",
    "    # Receive the results from API\n",
    "    activity_content = requests.get(activity_url).json()\n",
    "\n",
    "    # Perform a conversion from JSON to Data Frame\n",
    "    if 'path' in activity_content:\n",
    "        activity_df = json_normalize(activity_content['path'])\n",
    "    \n",
    "    # Add the activity path data to geolocation\n",
    "    geolocation_data = pd.concat([geolocation_data, activity_df])\n",
    "    \n",
    "geolocation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocation data import by period [less heavy]\n",
    "\n",
    "#### Note: Execute the dataSet filter first\n",
    "\n",
    "The cell below perform multiple requests to Runkeep Health Graph API, in order to retrieve all activities in details to get location informations such as: latitude/longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Geolocation data\n",
    "# =================================================================================\n",
    "\n",
    "# Copy the data\n",
    "geolocation_data = pd.DataFrame()\n",
    "\n",
    "# =================================================================================\n",
    "# API single activity request\n",
    "# =================================================================================\n",
    "\n",
    "# Base URI\n",
    "base_URI = \"http://api.runkeeper.com\"\n",
    "\n",
    "# Run through filtered activities\n",
    "for idx, row in tqdm(data_filter.iterrows()):\n",
    "    \n",
    "    # Final activity URI\n",
    "    activity_url = base_URI + row['uri'] + \"?access_token=\" + ACCESS_TOKEN\n",
    "\n",
    "    # Receive the results from API\n",
    "    activity_content = requests.get(activity_url).json()\n",
    "\n",
    "    # Perform a conversion from JSON to Data Frame\n",
    "    if 'path' in activity_content:\n",
    "        activity_df = json_normalize(activity_content['path'])\n",
    "\n",
    "    # Add the activity path data to geolocation\n",
    "    geolocation_data = pd.concat([geolocation_data, activity_df])\n",
    "    \n",
    "geolocation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export [optional]\n",
    "\n",
    "In order to avoid processing the cell above, here we are saving the processed geolocationd data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the new dataSet to csv\n",
    "geolocation_data.to_csv('geolocation2018.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
